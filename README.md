# datasets
This is a list of datasets and other resources which may be useful for machine learning applications in visual effects (VFX). Some of these are free for commercial use, others are not. So, please, take the time to read any associated license agreements before utilising one in your own project. Contributions, corrections, and curation welcome.

[vfx.ai](http://vfx.ai/2017/12/vfx-datasets/)

## 2D/Compositing
* [Alpha Matting dataset](http://alphamatting.com/datasets.php)
* [Common Objects in Context (COCO)](http://cocodataset.org/#explore)
* [Video Matting dataset](http://videomatting.com/#datasets)
* [Deep Automatic Portrait Mapping dataset](http://www.cse.cuhk.edu.hk/leojia/projects/automatting/index.html)
* [PortraitFCN](http://xiaoyongshen.me/webpage_portrait/index.html)
* [MSRA10K Salient Object Database](http://mmcheng.net/msra10k/)
* [MPI Sintel Optical Flow Dataset](http://sintel.is.tue.mpg.de)
* [Flying Chairs Optical Flow Dataset](https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html)
* [RENOIR - A Dataset for Real Low-Light Image Noise Reduction](http://ani.stat.fsu.edu/~abarbu/Renoir.html)
* [Darmstadt Noise Dataset](https://noise.visinf.tu-darmstadt.de)
* [GOPRO deblur dataset](https://github.com/SeungjunNah/DeepDeblur_release)
* [Kohler deblur dataset](http://webdav.is.mpg.de/pixel/benchmark4camerashake/)
* [HDR image reconstruction from a single exposure using deep CNNs testset](http://hdrv.org/hdrcnn/material/testset/index.html)
* [NYU Depth Dataset](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)

## Crowd Simulation
* [CUHK Crowd Dataset](http://www.ee.cuhk.edu.hk/~jshao/CUHKcrowd_files/cuhk_crowd_dataset.htm)

## Models
* [ShapeNet](https://www.shapenet.org)
* [ModelNet](http://modelnet.cs.princeton.edu)
* [Shape Retrival Contest (SHREC) Datasets](http://tosca.cs.technion.ac.il/book/shrec.html)
* [TOSCA high-resolution and Non-rigid world](http://tosca.cs.technion.ac.il/book/resources_data.html)
* [Large Geometric Models Archive](https://www.cc.gatech.edu/projects/large_models/)
* [The CAnonically Posed 3D Objects Dataset](https://sites.google.com/site/pgpapadakis/home/CAPOD)
* [MIT CSAIL Textured Models Database](http://people.csail.mit.edu/tmertens/textransfer/data/)
* [PASCAL3D+](http://cvgl.stanford.edu/projects/pascal3d.html)
* [Stanford 3D Scanning Repository](http://graphics.stanford.edu/data/3Dscanrep/)

## Miscellaneous
* [Robust Global Translations with 1DSfM](http://www.cs.cornell.edu/projects/1dsfm/)
* [CMP Facade database](http://cmp.felk.cvut.cz/~tylecr1/facade/)
* [SUN360 panorama database](http://people.csail.mit.edu/jxiao/SUN360/)
* [Image Aesthetic Visual Analysis (AVA)](https://github.com/shubhamchaudhary/aesthetics)
* [CUHKPQ Photo quality dataset](http://mmlab.ie.cuhk.edu.hk/archive/CUHKPQ/Dataset.htm)


## Motion Capture

### Body
* [CMU Graphics Lab Motion Capture Database](http://mocap.cs.cmu.edu)
* [MoCap Hand Postures Data Set](https://archive.ics.uci.edu/ml/datasets/MoCap+Hand+Postures)
* [SCAPE: Shape Completion and Animation of People](http://ai.stanford.edu/~drago/Projects/scape/scape.html)
* [ACCAD Motion Capture Lab Library](https://accad.osu.edu/research/mocap/mocap_data.htm)
* [mocapdata.com](http://mocapdata.com)
* [The Motion Capture Club Library](http://mocapclub.com)
* [Body Movement Library](http://paco.psy.gla.ac.uk/index.php?option=com_jdownloads&view=viewcategories&Itemid=62)
* [KU Leuven Action Database](https://ppw.kuleuven.be/home/english/research/lep/resources/action)
* [Emotional Body Motion Database](http://ebmdb.tuebingen.mpg.de/index.php)
* [Multiple Human Pose Estimation dataset](http://campar.in.tum.de/Chair/MultiHumanPose)
* [The Kinetics Human Action Video Dataset](https://deepmind.com/research/open-source/open-source-datasets/kinetics/)
* [UBC3V](https://github.com/ashafaei/ubc3v)
* [Human3.6M](http://vision.imar.ro/human3.6m/description.php)
* [MoSh Dataset](http://mosh.is.tue.mpg.de)
* [JHMDB Dataset](http://jhmdb.is.tue.mpg.de)
* [Penn Action Dataset](http://dreamdragon.github.io/PennAction/)

### Eyes
* [GazeCapture](http://gazecapture.csail.mit.edu)
* [Columbia Gaze Data Set](http://www.cs.columbia.edu/CAVE/databases/columbia_gaze/)
* [MPIIGaze Dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/)
* [High Quality Eye Capture Dataset](https://www.disneyresearch.com/publication/high-quality-capture-of-eyes/)
* [SynthesEyes Dataset](https://www.cl.cam.ac.uk/research/rainbow/projects/syntheseyes/)
* [Kaggle Eye Gaze](https://www.kaggle.com/4quant/eye-gaze)
* [YouTube Faces Dataset with Facial Keypoints](https://www.kaggle.com/selfishgene/youtube-faces-with-facial-keypoints)
* [Swirski pupil dataset](https://www.cl.cam.ac.uk/research/rainbow/projects/pupiltracking/)
* [ExCuSe/ElSe pupil datasets](http://www.ti.uni-tuebingen.de/Pupil-detection.1827.0.html)
* [Labeled pupils in the wild (LPW)](https://perceptual.mpi-inf.mpg.de/research/datasets/)
* [Osnabruck gaze dataset](https://portal.ikw.uni-osnabrueck.de/~cv/index.php?open=projects/mm-mkv)
* [Closed Eyes In The Wild (CEW)](http://parnec.nuaa.edu.cn/xtan/data/ClosedEyeDatabases.html)

### Face
* [FaceWarehouse](http://gaps-zju.org/facewarehouse/)
* [NVIDIA Synthetic Head Dataset](https://docs.google.com/forms/d/e/1FAIpQLSe6EP2FznQmlIf-QP8VJ638p9Da4wiZz2u-iTGtDgWV0qzXOA/viewform)
* [CASIA 3D Face Database](http://www.cbsr.ia.ac.cn/english/3DFace%20Databases.asp)
* [University of York 3D Face Database](https://www-users.cs.york.ac.uk/nep/research/3Dface/tomh/3DFaceDatabase.html)
* [Face Database of the Max Planck Institute for Biological Cybernetics](http://faces.kyb.tuebingen.mpg.de/index.php)
* [The Bosphorus Database](http://bosphorus.ee.boun.edu.tr/default.aspx)
* [Spacetime Faces](http://grail.cs.washington.edu/projects/stfaces/)
* [ETH Face Pose Range Image Data Set](https://www.vision.ee.ethz.ch/datasets/headposeCVPR08/)
* [Biwi Kinect Head Pose Database](https://data.vision.ee.ethz.ch/cvl/gfanelli/head_pose/head_forest.html#db)
* [BU-3DFE, BU-4DFE, and BP4D-Spontaneous](http://www.cs.binghamton.edu/~lijun/Research/3DFE/3DFE_Analysis.html)
* [Eurecom Kinect Face Dataset](http://rgb-d.eurecom.fr)
* [CMU Face Images Data Set](http://archive.ics.uci.edu/ml/datasets/cmu+face+images)
* [Large Scale Facial Model](https://ibug.doc.ic.ac.uk/resources/lsfm/)

## Scenes
* [SceneNet](https://robotvault.bitbucket.io)
* [SUNCG](http://suncg.cs.princeton.edu)
* [3D Indoor Scenes Database](https://toolkit.cit-ec.uni-bielefeld.de/datasets/3d-indoor-scenes-database)
* [ScanNet](http://www.scan-net.org)

## Shading / Lighting
* [MERL BRDF database](https://www.merl.com/brdf/)
* [UTIA BTF database](http://btf.utia.cas.cz)
* [Columbia-Utrecht Reflectance and Texture Database](http://www.cs.columbia.edu/CAVE/software/curet/)
* [Weather and Illumination Database](http://www1.cs.columbia.edu/CAVE/software/wild/index.php)
* [Fabrics Dataset](https://ibug.doc.ic.ac.uk/resources/fabrics/)

## Simulation
* [John Hopkins Turbulence Databases](http://turbulence.pha.jhu.edu)

## Volumetric
* [The Volume Library](http://lgdv.cs.fau.de/External/vollib/)

